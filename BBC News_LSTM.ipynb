{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (4.43.0)\n",
      "Requirement already satisfied: regex in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the hyparameters at the top like this to make it easier to change and edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define two lists that containing articles and labels. In the meantime, we remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)\n",
    "print(len(labels))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 2,225 articles in the data. Then we split into training set and validation set, according to the parameter we set earlier, 80% for training, 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n",
      "1780\n",
      "1780\n",
      "445\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer does all the heavy lifting for us. In our articles that it was tokenizing, it will take 5,000 most common words. oov_token is to put a special value in when an unseen word is encountered. This means I want \"OOV\" in bracket to be used to for words that are not in the word index. \"fit_on_text\" will go through all the text and create dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that \"OOV\" in bracket is number 1, \"said\" is number 2, \"mr\" is number 3, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'said': 2,\n",
       " 'mr': 3,\n",
       " 'would': 4,\n",
       " 'year': 5,\n",
       " 'also': 6,\n",
       " 'people': 7,\n",
       " 'new': 8,\n",
       " 'us': 9,\n",
       " 'one': 10}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process cleans up our text, lowercase, and remove punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, the next step is to turn thoes tokens into lists of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 11th article in the training data that has been turned into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2431, 8718, 225, 4996, 22, 641, 587, 225, 4996, 7154, 11247, 1661, 8719, 9796, 2431, 22, 565, 6120, 7155, 140, 278, 13483, 140, 278, 796, 823, 662, 2307, 7155, 1145, 1692, 17556, 1719, 4997, 13484, 6586, 7850, 17557, 13485, 4738, 9797, 6121, 122, 4514, 13486, 2, 2873, 1505, 352, 4739, 17558, 52, 341, 6122, 352, 2171, 3963, 41, 22, 3795, 17559, 17560, 17561, 7155, 542, 13487, 9798, 7156, 835, 631, 2366, 347, 4740, 7851, 365, 22, 13486, 787, 2367, 13488, 4302, 138, 10, 7155, 3664, 682, 3532, 6587, 22, 7155, 414, 823, 662, 7154, 90, 13, 633, 6586, 225, 4996, 7850, 599, 13489, 1692, 1021, 7850, 4998, 808, 1863, 117, 11248, 9797, 6121, 2973, 22, 8720, 99, 278, 7155, 1606, 4999, 542, 492, 7155, 1443, 4741, 778, 1320, 7852, 1860, 10, 33, 641, 319, 7157, 62, 478, 565, 301, 1506, 22, 479, 13490, 7853, 1664, 7158, 797, 6588, 3066, 11249, 1364, 6, 13491, 2431, 565, 22, 2970, 4735, 7158, 13492, 7158, 13493, 17562, 850, 39, 1824, 675, 297, 26, 979, 13494, 882, 22, 361, 22, 13, 301, 1506, 1342, 374, 20, 63, 883, 1097, 4303, 247]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train neural networks for NLP, we need sequences to be in the same size, that's why we use padding. Our max_length is 200, so we use pad_sequences to make all of our articles the same length which is 200 in my example. That's why you see that the 1st article was 426 in length, becomes 200, the 2nd article was 192 in length, becomes 200, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "200\n",
      "192\n",
      "200\n",
      "186\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addtion, there is padding type and truncating type, there are all \"post\". Means for example, for the 11th article, it was 186 in length, we padded to 200, and we padded at the end, add 14 zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2431, 8718, 225, 4996, 22, 641, 587, 225, 4996, 7154, 11247, 1661, 8719, 9796, 2431, 22, 565, 6120, 7155, 140, 278, 13483, 140, 278, 796, 823, 662, 2307, 7155, 1145, 1692, 17556, 1719, 4997, 13484, 6586, 7850, 17557, 13485, 4738, 9797, 6121, 122, 4514, 13486, 2, 2873, 1505, 352, 4739, 17558, 52, 341, 6122, 352, 2171, 3963, 41, 22, 3795, 17559, 17560, 17561, 7155, 542, 13487, 9798, 7156, 835, 631, 2366, 347, 4740, 7851, 365, 22, 13486, 787, 2367, 13488, 4302, 138, 10, 7155, 3664, 682, 3532, 6587, 22, 7155, 414, 823, 662, 7154, 90, 13, 633, 6586, 225, 4996, 7850, 599, 13489, 1692, 1021, 7850, 4998, 808, 1863, 117, 11248, 9797, 6121, 2973, 22, 8720, 99, 278, 7155, 1606, 4999, 542, 492, 7155, 1443, 4741, 778, 1320, 7852, 1860, 10, 33, 641, 319, 7157, 62, 478, 565, 301, 1506, 22, 479, 13490, 7853, 1664, 7158, 797, 6588, 3066, 11249, 1364, 6, 13491, 2431, 565, 22, 2970, 4735, 7158, 13492, 7158, 13493, 17562, 850, 39, 1824, 675, 297, 26, 979, 13494, 882, 22, 361, 22, 13, 301, 1506, 1342, 374, 20, 63, 883, 1097, 4303, 247]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2431  8718   225  4996    22   641   587   225  4996  7154 11247  1661\n",
      "  8719  9796  2431    22   565  6120  7155   140   278 13483   140   278\n",
      "   796   823   662  2307  7155  1145  1692 17556  1719  4997 13484  6586\n",
      "  7850 17557 13485  4738  9797  6121   122  4514 13486     2  2873  1505\n",
      "   352  4739 17558    52   341  6122   352  2171  3963    41    22  3795\n",
      " 17559 17560 17561  7155   542 13487  9798  7156   835   631  2366   347\n",
      "  4740  7851   365    22 13486   787  2367 13488  4302   138    10  7155\n",
      "  3664   682  3532  6587    22  7155   414   823   662  7154    90    13\n",
      "   633  6586   225  4996  7850   599 13489  1692  1021  7850  4998   808\n",
      "  1863   117 11248  9797  6121  2973    22  8720    99   278  7155  1606\n",
      "  4999   542   492  7155  1443  4741   778  1320  7852  1860    10    33\n",
      "   641   319  7157    62   478   565   301  1506    22   479 13490  7853\n",
      "  1664  7158   797  6588  3066 11249  1364     6 13491  2431   565    22\n",
      "  2970  4735  7158 13492  7158 13493 17562   850    39  1824   675   297\n",
      "    26   979 13494   882    22   361    22    13   301  1506  1342   374\n",
      "    20    63   883  1097  4303   247     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the 1st article, it was 426 in length, we truncated to 200, and we truncated at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91, 160, 1142, 1107, 49, 979, 755, 6569, 89, 1304, 4289, 129, 175, 3653, 1214, 1195, 1575, 42, 7, 893, 91, 6099, 334, 85, 20, 14, 130, 3261, 1215, 2421, 570, 451, 1375, 58, 3379, 3522, 1659, 8, 921, 730, 10, 844, 17503, 9, 598, 1576, 1108, 395, 1940, 1107, 731, 49, 537, 1397, 2010, 1621, 134, 249, 113, 2355, 795, 4981, 980, 584, 10, 3958, 3959, 921, 2562, 129, 344, 175, 3653, 7822, 5317, 39, 62, 2866, 28, 9, 4723, 18, 1305, 136, 416, 7, 143, 1422, 71, 4501, 436, 4982, 91, 1108, 77, 6100, 82, 2011, 53, 7823, 91, 6, 1008, 609, 89, 1304, 91, 1963, 131, 137, 420, 9, 2867, 38, 152, 1235, 89, 1304, 4724, 7, 436, 4982, 3153, 6, 2492, 8701, 431, 1127, 7824, 1423, 571, 1216, 1901, 13452, 766, 9, 537, 1397, 2010, 134, 2068, 400, 845, 1964, 1599, 34, 1715, 2868, 5318, 5319, 2422, 244, 9, 2624, 82, 732, 6, 1173, 1196, 152, 720, 591, 9773, 124, 28, 1305, 1688, 432, 83, 933, 115, 20, 14, 18, 3154, 17504, 37, 1484, 13453, 23, 37, 87, 335, 2356, 37, 467, 255, 1964, 1358, 328, 5700, 299, 732, 1174, 18, 2869, 1715, 5319, 294, 756, 1075, 395, 2012, 387, 431, 2012, 2, 1359, 11225, 1715, 2165, 67, 17505, 17506, 1716, 249, 1660, 3059, 1175, 395, 41, 878, 246, 2792, 345, 53, 548, 400, 2, 17507, 17508, 655, 1360, 202, 91, 3960, 91, 90, 42, 7, 320, 395, 77, 893, 13454, 91, 1107, 400, 537, 9, 845, 2422, 11, 38, 9774, 995, 513, 483, 2069, 160, 572, 17509, 128, 7, 320, 77, 893, 1217, 1127, 1464, 346, 54, 2213, 1218, 741, 92, 256, 274, 1019, 71, 623, 346, 2423, 756, 1215, 2357, 1717, 6101, 3783, 3523, 7824, 1127, 2012, 177, 371, 1398, 77, 53, 548, 105, 1142, 3, 11225, 1048, 93, 2961, 17510, 2625, 9775, 102, 902, 440, 452, 2, 3, 11225, 2870, 451, 1424, 43, 77, 429, 31, 8, 1019, 921, 6570, 2562, 30, 6102, 91, 1689, 879, 89, 1304, 91, 1963, 7136, 30, 8, 1622, 7137, 7138, 4290, 1577, 4289, 656, 7822, 3784, 1008, 572, 4291, 2866, 10, 880, 656, 58, 17511, 1262, 5320, 7137, 91, 1552, 934, 4723, 7822, 577, 4106, 10, 9, 235, 2010, 91, 134, 17512, 95, 656, 3262, 7822, 58, 519, 673, 2626, 3784, 4983, 3380, 483, 4725, 39, 4501, 8702, 91, 1746, 673, 269, 116, 239, 2627, 354, 643, 58, 4107, 757, 3654, 4723, 146, 17513, 400, 7, 71, 1747, 1108, 767, 910, 118, 584, 3381, 1316, 1576, 9776, 1600, 7, 893, 77, 77]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   91   160  1142  1107    49   979   755  6569    89  1304  4289   129\n",
      "   175  3653  1214  1195  1575    42     7   893    91  6099   334    85\n",
      "    20    14   130  3261  1215  2421   570   451  1375    58  3379  3522\n",
      "  1659     8   921   730    10   844 17503     9   598  1576  1108   395\n",
      "  1940  1107   731    49   537  1397  2010  1621   134   249   113  2355\n",
      "   795  4981   980   584    10  3958  3959   921  2562   129   344   175\n",
      "  3653  7822  5317    39    62  2866    28     9  4723    18  1305   136\n",
      "   416     7   143  1422    71  4501   436  4982    91  1108    77  6100\n",
      "    82  2011    53  7823    91     6  1008   609    89  1304    91  1963\n",
      "   131   137   420     9  2867    38   152  1235    89  1304  4724     7\n",
      "   436  4982  3153     6  2492  8701   431  1127  7824  1423   571  1216\n",
      "  1901 13452   766     9   537  1397  2010   134  2068   400   845  1964\n",
      "  1599    34  1715  2868  5318  5319  2422   244     9  2624    82   732\n",
      "     6  1173  1196   152   720   591  9773   124    28  1305  1688   432\n",
      "    83   933   115    20    14    18  3154 17504    37  1484 13453    23\n",
      "    37    87   335  2356    37   467   255  1964  1358   328  5700   299\n",
      "   732  1174    18  2869  1715  5319   294   756]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same for the validation sequences. Note that we should expect more out of vocabulary words from validation articles because word index were derived from the training articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "(445, 200)\n"
     ]
    }
   ],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at the labels. because our labels are text, so we will tokenize them, when training, labels are expected to be numpy arrays. So we will turn list of labels into numpy arrays like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entertainment', 'business', 'tech', 'sport', 'politics'}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[2]\n",
      "[1]\n",
      "(1780, 1)\n",
      "[5]\n",
      "[4]\n",
      "[3]\n",
      "(445, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_label_seq[0])\n",
    "print(training_label_seq[1])\n",
    "print(training_label_seq[2])\n",
    "print(training_label_seq.shape)\n",
    "\n",
    "print(validation_label_seq[0])\n",
    "print(validation_label_seq[1])\n",
    "print(validation_label_seq[2])\n",
    "print(validation_label_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training deep neural network, we want to explore what our original article and article after padding look like. Running the following code, we explore the 11th article, we can see that some words become \"OOV\", because they did not make to the top 5,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin cheers anti nazi film german movie anti nazi resistance heroine drawn loud applause berlin film festival sophie scholl final days portrays final days member white rose movement scholl 21 arrested beheaded brother hans 1943 distributing leaflets condemning abhorrent tyranny adolf hitler director marc rothemund said feeling responsibility keep legacy scholls going must somehow keep ideas alive added film drew transcripts gestapo interrogations scholl trial preserved archive communist east germany secret police discovery inspiration behind film rothemund worked closely surviving relatives including one scholl sisters ensure historical accuracy film scholl members white rose resistance group first started distributing anti nazi leaflets summer 1942 arrested dropped leaflets munich university calling day reckoning adolf hitler regime film focuses six days scholl arrest intense trial saw scholl initially deny charges ended defiant appearance one three german films vying top prize festival south african film version bizet tragic opera carmen shot cape town xhosa language also premiered berlin festival film entitled u carmen ekhayelitsha carmen khayelitsha township story set performed 40 strong music theatre troupe debut film performance film first south african feature 25 years second nominated golden bear award ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "---\n",
      "berlin cheers anti-nazi film german movie anti-nazi resistance heroine drawn loud applause berlin film festival.  sophie scholl - final days portrays final days member white rose movement. scholl  21  arrested beheaded brother  hans  1943 distributing leaflets condemning  abhorrent tyranny  adolf hitler. director marc rothemund said:  feeling responsibility keep legacy scholls going.   must somehow keep ideas alive   added.  film drew transcripts gestapo interrogations scholl trial preserved archive communist east germany secret police. discovery inspiration behind film rothemund  worked closely surviving relatives  including one scholl sisters  ensure historical accuracy film. scholl members white rose resistance group first started distributing anti-nazi leaflets summer 1942. arrested dropped leaflets munich university calling  day reckoning  adolf hitler regime. film focuses six days scholl arrest intense trial saw scholl initially deny charges ended defiant appearance. one three german films vying top prize festival.  south african film version bizet tragic opera carmen shot cape town xhosa language also premiered berlin festival. film entitled u-carmen ekhayelitsha carmen khayelitsha township story set. performed 40-strong music theatre troupe debut film performance. film first south african feature 25 years second nominated golden bear award.\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_padded[10]))\n",
    "print('---')\n",
    "print(train_articles[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement LSTM. Here is my code that I build a tf.keras.Sequential model and start with an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices into sequences of vectors. After training, words with similar meanings often have the similar vectors.\n",
    "\n",
    "Next is how to implement LSTM in code. The Bidirectional wrapper is used with a LSTM layer, this propagates the input forwards and backwards through the LSTM layer and then concatenates the outputs. This helps LSTM to learn long term dependencies. We then fit it to a dense neural network to do classification.\n",
    "\n",
    "This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a tf.keras.layers.Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,354,694\n",
      "Trainable params: 1,354,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model summay, we have our embeddings, our Bidirectional contains LSTM, followed by two dense layers. The output from Bidirectional is 128, because it doubled what we put in LSTM. We can also stack LSTM layer but I found the results worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 - 6s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9393\n",
      "Epoch 2/20\n",
      "56/56 - 6s - loss: 6.8062e-04 - accuracy: 1.0000 - val_loss: 0.2353 - val_accuracy: 0.9393\n",
      "Epoch 3/20\n",
      "56/56 - 6s - loss: 4.6482e-04 - accuracy: 1.0000 - val_loss: 0.2338 - val_accuracy: 0.9416\n",
      "Epoch 4/20\n",
      "56/56 - 6s - loss: 3.2936e-04 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9393\n",
      "Epoch 5/20\n",
      "56/56 - 6s - loss: 2.4279e-04 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9393\n",
      "Epoch 6/20\n",
      "56/56 - 6s - loss: 1.8737e-04 - accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9393\n",
      "Epoch 7/20\n",
      "56/56 - 6s - loss: 1.4999e-04 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9393\n",
      "Epoch 8/20\n",
      "56/56 - 6s - loss: 1.2438e-04 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9393\n",
      "Epoch 9/20\n",
      "56/56 - 6s - loss: 1.0606e-04 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9416\n",
      "Epoch 10/20\n",
      "56/56 - 6s - loss: 9.1878e-05 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9416\n",
      "Epoch 11/20\n",
      "56/56 - 6s - loss: 8.0751e-05 - accuracy: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9416\n",
      "Epoch 12/20\n",
      "56/56 - 6s - loss: 7.1612e-05 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9416\n",
      "Epoch 13/20\n",
      "56/56 - 6s - loss: 6.4180e-05 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 0.9416\n",
      "Epoch 14/20\n",
      "56/56 - 6s - loss: 5.7920e-05 - accuracy: 1.0000 - val_loss: 0.2493 - val_accuracy: 0.9416\n",
      "Epoch 15/20\n",
      "56/56 - 6s - loss: 5.2568e-05 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9416\n",
      "Epoch 16/20\n",
      "56/56 - 6s - loss: 4.7973e-05 - accuracy: 1.0000 - val_loss: 0.2522 - val_accuracy: 0.9416\n",
      "Epoch 17/20\n",
      "56/56 - 6s - loss: 4.3965e-05 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9416\n",
      "Epoch 18/20\n",
      "56/56 - 6s - loss: 4.0469e-05 - accuracy: 1.0000 - val_loss: 0.2552 - val_accuracy: 0.9438\n",
      "Epoch 19/20\n",
      "56/56 - 6s - loss: 3.7379e-05 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9438\n",
      "Epoch 20/20\n",
      "56/56 - 6s - loss: 3.4638e-05 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 0.9438\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\win10\\.conda\\envs\\tensorflow2\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1468757e-08, 9.9999487e-01, 3.2503504e-08, 1.0811288e-07,\n",
       "        1.0476536e-08, 5.0495801e-06]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = [\"A WeWork shareholder has taken the company to court over the near-$1.7bn (1.3bn) leaving package approved for ousted co-founder Adam Neumann.\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1433118e-08 9.9993944e-01 1.1084168e-07 3.7909661e-08 9.5222701e-09\n",
      "  6.0458875e-05]] bussiness\n"
     ]
    }
   ],
   "source": [
    "txt = [\"yeading face newcastle in fa cup premiership side newcastle united face a trip to ryman premier league leaders yeading in the fa cup third round.  the game - arguably the highlight of the draw - is a potential money-spinner for non-league yeading  who beat slough in the second round. conference side exeter city  who knocked out doncaster on saturday  will travel to old trafford to meet holders manchester united in january. arsenal were drawn at home to stoke and chelsea will play host to scunthorpe. the only other non-league side in the draw are hinckley united  who held brentford to a goalless draw on sunday. they will meet league one leaders luton if they win their replay against martin allen s team at griffin park.  a number of premiership teams face difficult away games against championship sides on the weekend of 8/9 january. third-placed everton visit plymouth  liverpool travel to burnley  crystal palace go to sunderland  fulham face carling cup semi-finalists watford  bolton meet ipswich  while aston villa were drawn against sheffield united. premiership strugglers norwich  blackburn  west brom are away at west ham  cardiff and preston north end respectively. southampton visit northampton  having already beaten the league two side in the carling cup earlier this season. middlesbrough were drawn away against either swindon or notts county  while spurs entertain brighton at white hart lane.  arsenal v stoke  swindon/notts co v middlesbrough  man utd v exeter  plymouth v everton  leicester v blackpool  derby v wigan  sunderland v crystal palace  wolves v millwall  yeading v newcastle  hull v colchester  tottenham v brighton  reading v stockport/swansea  birmingham v leeds  hartlepool v boston  milton keynes dons v peterborough  oldham v man city  chelsea v scunthorpe  cardiff v blackburn  charlton v rochdale  west ham v norwich  sheff utd v aston villa  preston v west brom  rotherham v yeovil  burnley v liverpool  bournemouth v chester  coventry v crewe  watford v fulham  ipswich v bolton  portsmouth v gillingham  northampton v southampton  qpr v nottm forest  luton v hinckley/brentford  matches to be played on weekend of 8/9 january.\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment','unknown']\n",
    "#{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
